{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb512fb8",
   "metadata": {},
   "source": [
    "Importe les bibliothèques nécessaires pour les calculs numériques (numpy), la visualisation (matplotlib, mplot3d pour les graphiques 3D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd74132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSNE:\n",
    "    def __init__(self, n_components=2, perplexity=30.0, \n",
    "                 early_exaggeration=12.0, learning_rate=200.0, \n",
    "                 n_iter=1000, min_grad_norm=1e-7, \n",
    "                 random_state=None, verbose=0):\n",
    "         \"\"\"\n",
    "        Implémentation from scratch de t-SNE.\n",
    "        \n",
    "        Paramètres:\n",
    "        -----------\n",
    "        n_components : int, (défaut: 2)\n",
    "            Dimension de l'espace embarqué\n",
    "            \n",
    "        perplexity : float, (défaut: 30)\n",
    "            Contrôle le nombre de voisins locaux considérés\n",
    "            \n",
    "        early_exaggeration : float, (défaut: 12.0)\n",
    "            Facteur d'exagération initial pour séparer les clusters\n",
    "            \n",
    "        learning_rate : float, (défaut: 200.0)\n",
    "            Taux d'apprentissage pour la descente de gradient\n",
    "            \n",
    "        n_iter : int, (défaut: 1000)\n",
    "            Nombre maximal d'itérations\n",
    "            \n",
    "        min_grad_norm : float, (défaut: 1e-7)\n",
    "            Seuil minimal de la norme du gradient pour continuer\n",
    "            \n",
    "        random_state : int ou None, (défaut: None)\n",
    "            Graine pour le générateur aléatoire\n",
    "            \n",
    "        verbose : int, (défaut: 0)\n",
    "            Niveau de verbosité (0: silencieux, 1: progress, 2: détaillé)\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.perplexity = perplexity\n",
    "        self.early_exaggeration = early_exaggeration\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.min_grad_norm = min_grad_norm\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Résultats\n",
    "        self.embedding_ = None\n",
    "        self.kl_divergence_ = None\n",
    "        self.n_iter_ = None\n",
    "    \n",
    "        # Variables internes\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "            \n",
    "        '''Si une graine aléatoire est fournie, on l'utilise pour \n",
    "        initialiser le générateur de nombres aléatoires de NumPy \n",
    "        (pour reproductibilité).'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb63f2",
   "metadata": {},
   "source": [
    "Paramètres:\n",
    "-----------\n",
    "\n",
    "- `n_components`: Paramètre qui définit dans quelle dimension l'espace sera réduit (généralement 2 ou 3 pour la visualisation).\n",
    "\n",
    "\n",
    "- `perplexity`: Paramètre qui contrôle le nombre effectif de voisins considérés pour chaque point. Une valeur typique est entre 5 et 50.\n",
    "\n",
    "\n",
    "- `early_exaggeration`: Facteur qui exagère les distances au début de l'optimisation pour mieux séparer les clusters.\n",
    "\n",
    "\n",
    "- `learning_rate`: Contrôle la taille des pas dans la descente de gradient. Trop élevé peut causer une divergence, trop bas une convergence lente.\n",
    "\n",
    "\n",
    "- `n_iter`: Nombre maximum d'itérations pour l'optimisation.\n",
    "\n",
    "\n",
    "- `min_grad_norm`: Seuil en dessous duquel l'optimisation s'arrête (considérée comme convergée).\n",
    "\n",
    "\n",
    "- `random_state`: Graine aléatoire pour la reproductibilité des résultats.\n",
    "\n",
    "\n",
    "- `verbose`: Contrôle la quantité d'informations affichées pendant l'exécution.\n",
    "\n",
    "- Initialisation des variables qui stockeront les résultats:\n",
    "  - `embedding_`: La projection des données en dimension réduite\n",
    "  - `kl_divergence_`: La divergence KL finale (mesure de qualité)\n",
    "  - `n_iter_`: Le nombre d'itérations effectuées\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ca6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _euclidean_distance(self, X):\n",
    "    \"\"\"Calcule la matrice des distances euclidiennes carrées entre tous les points.\"\"\"\n",
    "    sum_X = np.sum(np.square(X), axis=1)\n",
    "    distances = np.add(-2 * np.dot(X, X.T), sum_X).T + sum_X\n",
    "    np.fill_diagonal(distances, 0.0)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777d634",
   "metadata": {},
   "source": [
    " Méthode qui calcule efficacement les distances euclidiennes carrées entre toutes les paires de points:\n",
    "  1. `sum_X = np.sum(np.square(X), axis=1)` - Calcule la somme des carrés pour chaque point\n",
    "  2. `distances = np.add(-2 * np.dot(X, X.T), sum_X).T + sum_X` - Formule mathématique optimisée pour calculer ||x-y||² = ||x||² + ||y||² - 2x·y\n",
    "  3. `np.fill_diagonal(distances, 0.0)` - Met les distances à soi-même à 0\n",
    "  4. Retourne la matrice de distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce988b9",
   "metadata": {},
   "outputs": [],
   "source": [
    " def _binary_search_perplexity(self, distances, perplexity, tol=1e-5, max_iter=50):\n",
    "        \"\"\"Trouve les sigma appropriés pour obtenir la perplexité souhaitée.\"\"\"\n",
    "        n_samples = distances.shape[0]\n",
    "        P = np.zeros((n_samples, n_samples))\n",
    "        beta = np.ones((n_samples, 1))\n",
    "        log_perplexity = np.log(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee232906",
   "metadata": {},
   "source": [
    " Méthode qui trouve les paramètres sigma (via beta=1/(2σ²)) pour obtenir la perplexité désirée:\n",
    "  1. Initialise la matrice de probabilités P à zéro\n",
    "  2. Initialise beta (inverse de la variance) à 1 pour tous les points\n",
    "  3. Calcule le log de la perplexité cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3de036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ignore la diagonale (distance à soi-même = 0)\n",
    "        for i in range(n_samples):\n",
    "            beta_min = -np.inf\n",
    "            beta_max = np.inf\n",
    "            dist_i = distances[i, np.concatenate((np.r_[0:i], np.r_[i+1:n_samples]))]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5db7c7",
   "metadata": {},
   "source": [
    "Pour chaque point i:\n",
    "  1. Initialise les bornes de recherche binaire\n",
    "  2. Extrait les distances de i à tous les autres points (en excluant i lui-même)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            for _ in range(max_iter):\n",
    "                # Calcul des probabilités conditionnelles\n",
    "                P_i = np.exp(-dist_i * beta[i])\n",
    "                sum_Pi = np.sum(P_i)\n",
    "                \n",
    "                if sum_Pi == 0:\n",
    "                    sum_Pi = 1e-8\n",
    "                \n",
    "                # Calcul de l'entropie\n",
    "                H = np.log(sum_Pi) + beta[i] * np.sum(dist_i * P_i) / sum_Pi\n",
    "                P_i = P_i / sum_Pi\n",
    "                \n",
    "                # Ajustement de beta (précision binaire)\n",
    "                H_diff = H - log_perplexity\n",
    "                if np.abs(H_diff) < tol:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70639573",
   "metadata": {},
   "source": [
    "Recherche binaire pour trouver le bon beta:\n",
    "  1. Calcule les probabilités conditionnelles P(j|i)\n",
    "  2. Gère le cas où la somme est nulle pour éviter la division par zéro\n",
    "Calcule l'entropie H (qui doit correspondre à log(perplexity))\n",
    "- Normalise les probabilités\n",
    "- Calcule l'écart à la cible et arrête si suffisamment proche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca2b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                if H_diff > 0:\n",
    "                    beta_min = beta[i].copy()\n",
    "                    if beta_max == np.inf:\n",
    "                        beta[i] *= 2.0\n",
    "                    else:\n",
    "                        beta[i] = (beta[i] + beta_max) / 2.0\n",
    "                else:\n",
    "                    beta_max = beta[i].copy()\n",
    "                    if beta_min == -np.inf:\n",
    "                        beta[i] /= 2.0\n",
    "                    else:\n",
    "                        beta[i] = (beta[i] + beta_min) / 2.0\n",
    "            \n",
    "            # Remplir la matrice P\n",
    "            P[i, np.concatenate((np.r_[0:i], np.r_[i+1:n_samples]))] = P_i\n",
    "        \n",
    "        return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ca48b",
   "metadata": {},
   "source": [
    "Ajuste beta selon si l'entropie est trop haute ou trop basse:\n",
    "  - Si H > log(perplexity): besoin de diminuer beta (augmenter sigma)\n",
    "  - Si H < log(perplexity): besoin d'augmenter beta (diminuer sigma)\n",
    "\n",
    "  Stocke les probabilités trouvées dans la matrice P\n",
    "- Retourne la matrice de probabilités conditionnelles P(j|i)\n",
    "\n",
    "Cette implémentation montre les parties clés de t-SNE: calcul des distances, recherche des bonnes variances pour obtenir la perplexité souhaitée, et calcul des probabilités dans l'espace de départ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e98d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_joint_probabilities(self, X, perplexity):\n",
    "        \"\"\"Calcule les probabilités jointes p_ij.\"\"\"\n",
    "        # Calcul des distances euclidiennes carrées\n",
    "        distances = self._euclidean_distance(X)\n",
    "        \n",
    "        # Calcul des probabilités conditionnelles\n",
    "        P = self._binary_search_perplexity(distances, perplexity)\n",
    "        \n",
    "        # Symétrisation et normalisation\n",
    "        P = (P + P.T) / (2.0 * P.shape[0])\n",
    "        P = np.maximum(P, 1e-12)\n",
    "        \n",
    "        return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e65de13",
   "metadata": {},
   "source": [
    "Cette méthode calcule les probabilités jointes `p_ij` qui représentent les similarités entre les points dans l'espace de haute dimension. Voici ce que fait chaque partie:\n",
    "\n",
    "## 1. Calcul des distances euclidiennes carrées\n",
    "```python\n",
    "distances = self._euclidean_distance(X)\n",
    "```\n",
    "- **Action**: Calcule la matrice des distances euclidiennes carrées entre toutes les paires de points dans l'espace original\n",
    "- **Détail technique**: Utilise la formule optimisée `||x-y||² = ||x||² + ||y||² - 2x·y`\n",
    "- **Résultat**: Matrice carrée de taille (n_samples × n_samples) où chaque élément [i,j] contient la distance entre le point i et j\n",
    "\n",
    "## 2. Calcul des probabilités conditionnelles\n",
    "```python\n",
    "P = self._binary_search_perplexity(distances, perplexity)\n",
    "```\n",
    "- **Action**: Trouve les probabilités conditionnelles P(j|i) via recherche binaire\n",
    "- **Fonctionnement**:\n",
    "  - Pour chaque point i, trouve un σ (sigma) tel que la distribution de probabilité sur ses voisins ait la perplexité souhaitée\n",
    "  - La perplexité contrôle le nombre effectif de voisins considérés\n",
    "- **Résultat**: Matrice P où P[i,j] = probabilité que j soit un voisin de i\n",
    "\n",
    "## 3. Symétrisation des probabilités\n",
    "```python\n",
    "P = (P + P.T) / (2.0 * P.shape[0])\n",
    "```\n",
    "- **Problème**: Les P(j|i) ne sont pas symétriques (P(j|i) ≠ P(i|j))\n",
    "- **Solution**: On crée des probabilités jointes symétriques en faisant:\n",
    "  - Moyenne entre P(j|i) et P(i|j) via `(P + P.T)`\n",
    "  - Normalisation par `2*N` pour que la somme totale soit 1\n",
    "- **But**: Obtenir une mesure de similarité symétrique entre paires de points\n",
    "\n",
    "## 4. Éviter les valeurs numériques trop petites\n",
    "```python\n",
    "P = np.maximum(P, 1e-12)\n",
    "```\n",
    "- **Problème**: Certaines probabilités pourraient devenir trop proches de zéro\n",
    "- **Solution**: On impose une valeur minimale de 10⁻¹²\n",
    "- **But**: Éviter:\n",
    "  - Des divisions par zéro\n",
    "  - Des problèmes avec le calcul du logarithme dans la divergence KL\n",
    "  - Des instabilités numériques dans l'optimisation\n",
    "\n",
    "## Résultat final\n",
    "- Retourne une matrice de probabilités jointes symétrique P où:\n",
    "  - P[i,j] = probabilité que i et j soient voisins\n",
    "  - Plus P[i,j] est grand, plus les points sont similaires\n",
    "  - La somme totale des P[i,j] vaut 1\n",
    "\n",
    "  ## Importance dans t-SNE\n",
    "Ces probabilités P représentent la structure de voisinage dans l'espace de haute dimension que t-SNE tentera de préserver dans l'espace de faible dimension. La qualité de ce calcul influence directement la qualité de la visualisation finale.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def _compute_low_dimensional_probabilities(self, Y):\n",
    "        \"\"\"Calcule les probabilités q_ij en basse dimension.\"\"\"\n",
    "        distances = self._euclidean_distance(Y)\n",
    "        inv_distances = 1.0 / (1.0 + distances)\n",
    "        np.fill_diagonal(inv_distances, 0.0)\n",
    "        Q = inv_distances / np.sum(inv_distances)\n",
    "        Q = np.maximum(Q, 1e-12)\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8613d3",
   "metadata": {},
   "source": [
    "  def _compute_low_dimensional_probabilities(self, Y):\n",
    "- Calcule les probabilités q_ij en basse dimension.\n",
    "- **Finalité** : Calcule la matrice de similarité `Q` entre les points dans l'espace réduit (2D/3D).\n",
    "\n",
    "\n",
    "distances = self._euclidean_distance(Y)\n",
    "- **Effet** : Calcule les distances euclidiennes carrées entre tous les points dans l'espace réduit.\n",
    "- **Finalité** : Permet de mesurer à quel point les points sont proches dans la visualisation.\n",
    "\n",
    "\n",
    "    inv_distances = 1.0 / (1.0 + distances)\n",
    "- **Effet** : Applique une loi de Student (t-distribution) pour convertir les distances en similarités.\n",
    "- **Finalité** : Les points proches auront une similarité élevée (~1), les points éloignés une similarité faible (~0).\n",
    "\n",
    "\n",
    "    np.fill_diagonal(inv_distances, 0.0)\n",
    "- **Effet** : Met à zéro la diagonale (un point n'est pas similaire à lui-même).\n",
    "- **Finalité** : Évite les auto-similarités qui fausseraient la normalisation.\n",
    "\n",
    "\n",
    "    Q = inv_distances / np.sum(inv_distances)\n",
    "- **Effet** : Normalise les similarités pour obtenir une distribution de probabilité.\n",
    "- **Finalité** : `Q` représente maintenant la probabilité que deux points soient voisins en 2D/3D.\n",
    "\n",
    "\n",
    "    Q = np.maximum(Q, 1e-12)\n",
    "- **Effet** : Évite les valeurs trop petites (problèmes numériques).\n",
    "- **Finalité** : Garantit la stabilité des calculs (évite `log(0)`).\n",
    "\n",
    "\n",
    "    return Q\n",
    "- **Finalité** : Retourne la matrice `Q` qui sera utilisée pour calculer le gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd55692",
   "metadata": {},
   "outputs": [],
   "source": [
    " def _compute_gradient(self, P, Q, Y):\n",
    "        \"\"\"Calcule le gradient de la divergence KL par rapport à l'embedding.\"\"\"\n",
    "        n = Y.shape[0]\n",
    "        gradient = np.zeros_like(Y)\n",
    "        \n",
    "        # Calcul des termes (p_ij - q_ij) * (1 + ||y_i - y_j||²)^-1\n",
    "        dist = 1.0 / (1.0 + self._euclidean_distance(Y))\n",
    "        pq_diff = (P - Q) * dist\n",
    "        \n",
    "        # Calcul du gradient\n",
    "        for i in range(n):\n",
    "            gradient[i] = 4.0 * np.sum((Y[i] - Y) * pq_diff[:, i][:, np.newaxis], axis=0)\n",
    "        \n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa9250",
   "metadata": {},
   "source": [
    "def _compute_gradient(self, P, Q, Y):\n",
    "    \"\"\"Calcule le gradient de la divergence KL par rapport à l'embedding.\"\"\"\n",
    "- **Finalité** : Calcule comment ajuster les positions `Y` pour minimiser l'erreur entre `P` (hautes dimensions) et `Q` (basses dimensions).\n",
    "\n",
    "\n",
    "    n = Y.shape[0]\n",
    "- **Effet** : Stocke le nombre de points.\n",
    "\n",
    "\n",
    "    gradient = np.zeros_like(Y)\n",
    "- **Effet** : Initialise un gradient de même forme que `Y` (rempli de zéros).\n",
    "- **Finalité** : Stockera les dérivées de la divergence KL par rapport à chaque point.\n",
    "\n",
    "\n",
    "    dist = 1.0 / (1.0 + self._euclidean_distance(Y))\n",
    "- **Effet** : Recalcule les similarités en 2D/3D (comme dans `Q`).\n",
    "- **Finalité** : Utilisé pour pondérer les différences entre `P` et `Q`.\n",
    "\n",
    "\n",
    "    pq_diff = (P - Q) * dist\n",
    "- **Effet** : Calcule `(P - Q) × (similarité en 2D)`.\n",
    "- **Finalité** : Plus la similarité `Q` est éloignée de `P`, plus le gradient sera fort.\n",
    "\n",
    "\n",
    "    for i in range(n):\n",
    "        gradient[i] = 4.0 * np.sum((Y[i] - Y) * pq_diff[:, i][:, np.newaxis], axis=0)\n",
    "- **Effet** : Pour chaque point, calcule une force d'attraction/répulsion basée sur `(P - Q)`.\n",
    "- **Finalité** :  \n",
    "  - Si `P > Q` (trop éloignés en 2D), le point sera attiré.  \n",
    "  - Si `P < Q` (trop proches en 2D), le point sera repoussé.\n",
    "\n",
    "\n",
    "    return gradient\n",
    "- **Finalité** : Retourne le gradient qui sera utilisé pour déplacer les points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96386262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_kl_divergence(self, P, Q):\n",
    "        \"\"\"Calcule la divergence KL entre P et Q.\"\"\"\n",
    "        return np.sum(P * np.log(P / Q))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba4acd6",
   "metadata": {},
   "source": [
    "def _compute_kl_divergence(self, P, Q):\n",
    "    \"\"\"Calcule la divergence KL entre P et Q.\"\"\"\n",
    "- **Finalité** : Mesure à quel point `Q` (2D) est différent de `P` (haute dimension).\n",
    "\n",
    "\n",
    "    return np.sum(P * np.log(P / Q))\n",
    "- **Effet** :  \n",
    "  - Si `Q` est très différent de `P`, la divergence est grande.  \n",
    "  - Si `Q ≈ P`, la divergence est proche de 0.\n",
    "- **Finalité** : Utilisé pour suivre la qualité de l'embedding pendant l'optimisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae47b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X):\n",
    "        \"\"\"Fit le modèle aux données X.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Vérification des données\n",
    "        if n_samples < 3 * self.perplexity:\n",
    "            raise ValueError(f\"Le nombre d'échantillons ({n_samples}) doit être au moins 3 * perplexity ({3*self.perplexity})\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Calcul des probabilités jointes P...\")\n",
    "        \n",
    "        # Calcul des P en haute dimension\n",
    "        P = self._compute_joint_probabilities(X, self.perplexity)\n",
    "        P *= self.early_exaggeration\n",
    "        \n",
    "        # Initialisation aléatoire de Y\n",
    "        Y = 1e-4 * np.random.randn(n_samples, self.n_components).astype(np.float32)\n",
    "        \n",
    "        # Variables pour l'optimisation\n",
    "        previous_gradient = np.zeros_like(Y)\n",
    "        gains = np.ones_like(Y)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Optimisation de l'embedding...\")\n",
    "        \n",
    "        # Optimisation\n",
    "        for i in range(self.n_iter):\n",
    "            # Calcul des Q en basse dimension\n",
    "            Q = self._compute_low_dimensional_probabilities(Y)\n",
    "            \n",
    "            # Calcul du gradient\n",
    "            gradient = self._compute_gradient(P, Q, Y)\n",
    "            grad_norm = np.linalg.norm(gradient)\n",
    "            \n",
    "            # Mise à jour avec momentum\n",
    "            gains = (gains + 0.2) * ((gradient > 0) != (previous_gradient > 0)) + \\\n",
    "                    (gains * 0.8) * ((gradient > 0) == (previous_gradient > 0))\n",
    "            gains = np.clip(gains, 0.01, np.inf)\n",
    "            \n",
    "            previous_gradient = gradient.copy()\n",
    "            Y -= self.learning_rate * (gains * gradient)\n",
    "            \n",
    "            # Centrage des données\n",
    "            Y = Y - np.mean(Y, axis=0)\n",
    "            \n",
    "            # Calcul de la divergence KL\n",
    "            kl_div = self._compute_kl_divergence(P, Q)\n",
    "            \n",
    "            # Réduction de l'exagération après 100 itérations\n",
    "            if i == 100:\n",
    "                P /= self.early_exaggeration\n",
    "            \n",
    "            # Affichage des informations\n",
    "            if self.verbose >= 1 and i % 100 == 0:\n",
    "                print(f\"Iteration {i}: KL divergence = {kl_div:.4f}, Gradient norm = {grad_norm:.4f}\")\n",
    "                \n",
    "                if grad_norm < self.min_grad_norm:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Arrêt prématuré à l'itération {i}: norme du gradient trop faible\")\n",
    "                    break\n",
    "        \n",
    "        # Sauvegarde des résultats\n",
    "        self.embedding_ = Y\n",
    "        self.kl_divergence_ = kl_div\n",
    "        self.n_iter_ = i + 1\n",
    "        \n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3361f",
   "metadata": {},
   "source": [
    "def fit(self, X):\n",
    "    \"\"\"Fit le modèle aux données X.\"\"\"\n",
    "- **Finalité** : Entraîne le modèle t-SNE sur les données `X`.\n",
    "\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "- **Effet** : Stocke le nombre de points.\n",
    "\n",
    "\n",
    "    if n_samples < 3 * self.perplexity:\n",
    "        raise ValueError(\"...\")\n",
    "- **Effet** : Vérifie qu'il y a assez de points pour la perplexité choisie.\n",
    "- **Finalité** : Évite des calculs instables (trop peu de voisins).\n",
    "\n",
    "\n",
    "    P = self._compute_joint_probabilities(X, self.perplexity)\n",
    "- **Effet** : Calcule les similarités `P` en haute dimension.\n",
    "- **Finalité** : Ces similarités doivent être préservées en 2D/3D.\n",
    "\n",
    "\n",
    "    P *= self.early_exaggeration\n",
    "- **Effet** : Exagère les similarités au début (×12 par défaut).\n",
    "- **Finalité** : Aide à séparer les clusters tôt dans l'optimisation.\n",
    "\n",
    "\n",
    "    Y = 1e-4 * np.random.randn(n_samples, self.n_components)\n",
    "- **Effet** : Initialise aléatoirement les positions en 2D/3D.\n",
    "- **Finalité** : Point de départ pour l'optimisation.\n",
    "\n",
    "\n",
    "    previous_gradient = np.zeros_like(Y)\n",
    "    gains = np.ones_like(Y)\n",
    "- **Effet** : Initialise les variables pour le momentum.\n",
    "- **Finalité** : Accélère la convergence en évitant les oscillations.\n",
    "\n",
    "\n",
    "    for i in range(self.n_iter):\n",
    "        Q = self._compute_low_dimensional_probabilities(Y)\n",
    "- **Effet** : Calcule `Q` (similarités en 2D/3D) à chaque itération.\n",
    "\n",
    "\n",
    "        gradient = self._compute_gradient(P, Q, Y)\n",
    "- **Effet** : Calcule comment déplacer les points pour rapprocher `Q` de `P`.\n",
    "\n",
    "\n",
    "        gains = (gains + 0.2) * ((gradient > 0) != (previous_gradient > 0)) + \\\n",
    "                (gains * 0.8) * ((gradient > 0) == (previous_gradient > 0))\n",
    "- **Effet** : Ajuste dynamiquement les gains pour accélérer la descente.\n",
    "- **Finalité** : Si le gradient change de direction, augmente le gain. Sinon, le diminue.\n",
    "\n",
    "\n",
    "        Y -= self.learning_rate * (gains * gradient)\n",
    "- **Effet** : Met à jour les positions en suivant le gradient.\n",
    "- **Finalité** : Déplace les points pour minimiser la divergence KL.\n",
    "\n",
    "\n",
    "        Y = Y - np.mean(Y, axis=0)\n",
    "- **Effet** : Centre les données pour éviter la dérive.\n",
    "- **Finalité** : Garde l'embedding stable numériquement.\n",
    "\n",
    "\n",
    "        if i == 100:\n",
    "            P /= self.early_exaggeration\n",
    "- **Effet** : Après 100 itérations, arrête l'exagération.\n",
    "- **Finalité** : Permet un affinement plus précis des positions.\n",
    "\n",
    "\n",
    "        if grad_norm < self.min_grad_norm:\n",
    "            break\n",
    "- **Effet** : Arrête l'optimisation si le gradient est trop petit.\n",
    "- **Finalité** : Évite des itérations inutiles une fois convergé.\n",
    "\n",
    "\n",
    "    self.embedding_ = Y\n",
    "- **Finalité** : Stocke le résultat final (coordonnées 2D/3D).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "   def fit_transform(self, X):\n",
    "        \"\"\"Fit le modèle aux données et retourne l'embedding.\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.embedding_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ccb59",
   "metadata": {},
   "source": [
    "def fit_transform(self, X):\n",
    "    \"\"\"Fit le modèle et retourne l'embedding.\"\"\"\n",
    "- **Finalité** : Combine `fit(X)` et retourne directement l'embedding.\n",
    "\n",
    "\n",
    "    self.fit(X)\n",
    "    return self.embedding_\n",
    "- **Effet** : Entraîne le modèle et renvoie les coordonnées 2D/3D.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845caaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_test_data(n_samples=300, case='blobs', random_state=None):\n",
    "    \"\"\"Génère des données de test.\"\"\"\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    if case == 'blobs':\n",
    "        # Données groupées en clusters\n",
    "        centers = np.array([[1, 1, 1], [-1, -1, 1], [1, -1, -1], [-1, 1, -1]])\n",
    "        X = np.vstack([center + np.random.randn(n_samples//4, 3)*0.3 for center in centers])\n",
    "        y = np.repeat(np.arange(4), n_samples//4)\n",
    "    elif case == 'swiss_roll':\n",
    "        # Données en forme de rouleau suisse\n",
    "        t = 1.5 * np.pi * (1 + 2 * np.random.rand(n_samples))\n",
    "        X = np.vstack([t * np.cos(t), 10 * np.random.rand(n_samples), t * np.sin(t)]).T\n",
    "        y = (t // np.pi).astype(int)\n",
    "    else:\n",
    "        # Données linéairement séparables\n",
    "        X = np.random.randn(n_samples, 3)\n",
    "        X[:n_samples//2] += 1\n",
    "        X[n_samples//2:] -= 1\n",
    "        y = np.zeros(n_samples)\n",
    "        y[n_samples//2:] = 1\n",
    "    \n",
    "    # Normalisation\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95491703",
   "metadata": {},
   "source": [
    "Cette fonction génère des jeux de données synthétiques en 3D pour tester des algorithmes comme t-SNE. Voici ce que fait chaque partie du code :\n",
    "\n",
    "## Initialisation et contrôle aléatoire\n",
    "```python\n",
    "def generate_test_data(n_samples=300, case='blobs', random_state=None):\n",
    "    \"\"\"Génère des données de test.\"\"\"\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "```\n",
    "- **Fonction** : Crée des données de test avec 3 options différentes\n",
    "- **Paramètres** :\n",
    "  - `n_samples` : nombre total de points (300 par défaut)\n",
    "  - `case` : type de données ('blobs', 'swiss_roll' ou autre)\n",
    "  - `random_state` : pour reproductibilité des résultats\n",
    "\n",
    "## 1. Cas 'blobs' - Données groupées en clusters\n",
    "```python\n",
    "if case == 'blobs':\n",
    "    centers = np.array([[1, 1, 1], [-1, -1, 1], [1, -1, -1], [-1, 1, -1]])\n",
    "    X = np.vstack([center + np.random.randn(n_samples//4, 3)*0.3 for center in centers])\n",
    "    y = np.repeat(np.arange(4), n_samples//4)\n",
    "```\n",
    "- **Structure** : 4 clusters gaussiens centrés autour de points en 3D\n",
    "- **Génération** :\n",
    "  - Crée 4 centres dans l'espace 3D\n",
    "  - Pour chaque centre, génère `n_samples//4` points avec une distribution normale (bruit de 0.3)\n",
    "- **Labels** : Chaque cluster a un label différent (0 à 3)\n",
    "\n",
    "## 2. Cas 'swiss_roll' - Rouleau suisse\n",
    "```python\n",
    "elif case == 'swiss_roll':\n",
    "    t = 1.5 * np.pi * (1 + 2 * np.random.rand(n_samples))\n",
    "    X = np.vstack([t * np.cos(t), 10 * np.random.rand(n_samples), t * np.sin(t)]).T\n",
    "    y = (t // np.pi).astype(int)\n",
    "```\n",
    "- **Structure** : Une spirale 3D (comme un rouleau de papier suisse)\n",
    "- **Génération** :\n",
    "  - `t` : Paramètre qui suit la longueur de la spirale\n",
    "  - Coordonnées x et z : fonction cos/sin de t pour créer la spirale\n",
    "  - Coordonnée y : aléatoire pour \"étaler\" le rouleau\n",
    "- **Labels** : Basés sur la position angulaire (t // π)\n",
    "\n",
    "## 3. Cas par défaut - Données linéairement séparables\n",
    "```python\n",
    "else:\n",
    "    X = np.random.randn(n_samples, 3)\n",
    "    X[:n_samples//2] += 1\n",
    "    X[n_samples//2:] -= 1\n",
    "    y = np.zeros(n_samples)\n",
    "    y[n_samples//2:] = 1\n",
    "```\n",
    "- **Structure** : Deux groupes séparables linéairement\n",
    "- **Génération** :\n",
    "  - Tous les points suivent d'abord une distribution normale\n",
    "  - On décale la 1ère moitié vers +1\n",
    "  - On décale la 2nde moitié vers -1\n",
    "- **Labels** : 0 pour le premier groupe, 1 pour le second\n",
    "\n",
    "## Normalisation finale\n",
    "```python\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "return X, y\n",
    "```\n",
    "- **Standardisation** :\n",
    "  - Centre les données (moyenne = 0)\n",
    "  - Met à l'échelle (écart-type = 1)\n",
    "- **Retour** :\n",
    "  - `X` : matrice (n_samples × 3) des features\n",
    "  - `y` : vecteur (n_samples) des labels/clusters\n",
    "\n",
    "## Exemples d'utilisation\n",
    "```python\n",
    "# 1. Génération de clusters\n",
    "X, y = generate_test_data(case='blobs') \n",
    "\n",
    "# 2. Génération d'une spirale 3D\n",
    "X, y = generate_test_data(n_samples=500, case='swiss_roll')\n",
    "\n",
    "# 3. Données reproductibles\n",
    "X, y = generate_test_data(random_state=42)\n",
    "```\n",
    "\n",
    "Cette fonction est particulièrement utile pour :\n",
    "- Tester des algorithmes de réduction de dimension (t-SNE, PCA)\n",
    "- Visualiser comment différents types de données se projettent en 2D\n",
    "- Comparer des méthodes de clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(X, y, title, ax=None):\n",
    "    \"\"\"Visualise les résultats en 2D ou 3D.\"\"\"\n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        if X.shape[1] == 3:\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "        else:\n",
    "            ax = fig.add_subplot(111)\n",
    "    \n",
    "    if X.shape[1] == 2:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='viridis', alpha=0.7)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d2c26",
   "metadata": {},
   "source": [
    "Cette fonction permet de visualiser des données en 2D ou 3D avec un codage couleur selon les labels. Voici son fonctionnement :\n",
    "\n",
    "## Initialisation du graphique\n",
    "```python\n",
    "def plot_results(X, y, title, ax=None):\n",
    "    \"\"\"Visualise les résultats en 2D ou 3D.\"\"\"\n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        if X.shape[1] == 3:\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "        else:\n",
    "            ax = fig.add_subplot(111)\n",
    "```\n",
    "- **Paramètres** :\n",
    "  - `X` : matrice des données (2D ou 3D)\n",
    "  - `y` : vecteur des labels/clusters\n",
    "  - `title` : titre du graphique\n",
    "  - `ax` : axe matplotlib existant (optionnel)\n",
    "\n",
    "- **Fonctionnement** :\n",
    "  - Si aucun axe n'est fourni (`ax=None`), crée une nouvelle figure\n",
    "  - Détecte automatiquement si les données sont en 2D ou 3D (`X.shape[1]`)\n",
    "  - Pour les données 3D, initialise une projection 3D\n",
    "\n",
    "## Affichage des points\n",
    "```python\n",
    "    if X.shape[1] == 2:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='viridis', alpha=0.7)\n",
    "```\n",
    "- **2D** : \n",
    "  - Affiche les deux premières colonnes de X comme coordonnées x et y\n",
    "  - Utilise `y` pour le codage couleur\n",
    "  - `cmap='viridis'` : palette de couleurs\n",
    "  - `alpha=0.7` : transparence pour mieux voir les superpositions\n",
    "\n",
    "- **3D** :\n",
    "  - Affiche les trois colonnes comme coordonnées x, y et z\n",
    "  - Même principe de coloration que pour le 2D\n",
    "\n",
    "## Finalisation du graphique\n",
    "```python\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "```\n",
    "- Ajoute un titre au graphique\n",
    "- Active une grille pour mieux visualiser les positions\n",
    "\n",
    "## Exemples d'utilisation\n",
    "```python\n",
    "# Données 2D\n",
    "X_2d = np.random.rand(100, 2)\n",
    "y = np.random.randint(0, 3, 100)\n",
    "plot_results(X_2d, y, \"Données 2D aléatoires\")\n",
    "\n",
    "# Données 3D\n",
    "X_3d = np.random.rand(100, 3)\n",
    "plot_results(X_3d, y, \"Données 3D aléatoires\", ax=ax)  # Sur un axe existant\n",
    "```\n",
    "\n",
    "## Fonctionnalités clés\n",
    "1. **Adaptation automatique** : gère aussi bien le 2D que le 3D\n",
    "2. **Visualisation claire** : \n",
    "   - Couleurs par cluster/classe\n",
    "   - Transparence pour voir les densités\n",
    "   - Grille de référence\n",
    "3. **Flexibilité** : peut s'intégrer dans une figure existante\n",
    "\n",
    "Cette fonction est particulièrement utile pour :\n",
    "- Visualiser les résultats de t-SNE/PCA\n",
    "- Vérifier la qualité des clusters\n",
    "- Comparer différentes projections de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c9d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Génération des données\n",
    "    X, y = generate_test_data(300, case='blobs', random_state=42)\n",
    "    \n",
    "    # Visualisation des données originales\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    ax1 = fig.add_subplot(131)\n",
    "    plot_results(X[:, :2], y, \"Projection 2D originale\", ax1)\n",
    "    \n",
    "    ax2 = fig.add_subplot(132, projection='3d')\n",
    "    plot_results(X, y, \"Données originales 3D\", ax2)\n",
    "    \n",
    "    # Application de t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=30, \n",
    "                learning_rate=200, n_iter=1000,\n",
    "                random_state=42, verbose=1)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    # Visualisation des résultats\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    plot_results(X_tsne, y, f\"Donnée après t-SNE 2D (temps: {duration:.2f}s)\", ax3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Affichage des informations\n",
    "    print(\"\\nRésultats t-SNE:\")\n",
    "    print(f\"Divergence KL finale: {tsne.kl_divergence_:.4f}\")\n",
    "    print(f\"Itérations effectuées: {tsne.n_iter_}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9634fa",
   "metadata": {},
   "source": [
    "# Explication détaillée de la fonction `main()`\n",
    "\n",
    "Cette fonction principale démontre un workflow complet de visualisation de données avec t-SNE. Voici son fonctionnement étape par étape :\n",
    "\n",
    "## 1. Génération des données de test\n",
    "```python\n",
    "X, y = generate_test_data(300, case='blobs', random_state=42)\n",
    "```\n",
    "- **Action**: Crée un jeu de données synthétique\n",
    "- **Paramètres**:\n",
    "  - 300 points (`n_samples=300`)\n",
    "  - Type 'blobs' (4 clusters gaussiens en 3D)\n",
    "  - `random_state=42` pour la reproductibilité\n",
    "- **Retourne**:\n",
    "  - `X`: matrice 300x3 des features\n",
    "  - `y`: vecteur des labels (0 à 3)\n",
    "\n",
    "## 2. Configuration de la figure\n",
    "```python\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "```\n",
    "- Crée une figure large de 15 pouces par 5 pouces\n",
    "- Permettra d'afficher 3 graphiques côte à côte\n",
    "\n",
    "## 3. Visualisation des données originales\n",
    "### Projection 2D\n",
    "```python\n",
    "ax1 = fig.add_subplot(131)\n",
    "plot_results(X[:, :2], y, \"Projection 2D originale\", ax1)\n",
    "```\n",
    "- **131**: 1 ligne, 3 colonnes, 1ère position\n",
    "- Affiche seulement les 2 premières dimensions\n",
    "- Montre la perte d'information en 2D brute\n",
    "\n",
    "### Vue 3D complète\n",
    "```python\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "plot_results(X, y, \"Données originales 3D\", ax2)\n",
    "```\n",
    "- **132**: 1 ligne, 3 colonnes, 2ème position\n",
    "- `projection='3d'` pour l'affichage 3D\n",
    "- Montre la structure complète des données\n",
    "\n",
    "## 4. Application de t-SNE\n",
    "```python\n",
    "tsne = TSNE(n_components=2, perplexity=30, \n",
    "            learning_rate=200, n_iter=1000,\n",
    "            random_state=42, verbose=1)\n",
    "```\n",
    "- **Configuration t-SNE**:\n",
    "  - Réduction en 2D (`n_components=2`)\n",
    "  - Perplexité moyenne (`perplexity=30`)\n",
    "  - Taux d'apprentissage élevé (`learning_rate=200`)\n",
    "  - 1000 itérations maximum\n",
    "  - `verbose=1` pour afficher la progression\n",
    "\n",
    "### Calcul et chronométrage\n",
    "```python\n",
    "start_time = time.time()\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "duration = time.time() - start_time\n",
    "```\n",
    "- Mesure le temps d'exécution de t-SNE\n",
    "- `fit_transform()` applique l'algorithme et retourne la projection 2D\n",
    "\n",
    "## 5. Visualisation des résultats t-SNE\n",
    "```python\n",
    "ax3 = fig.add_subplot(133)\n",
    "plot_results(X_tsne, y, f\"Donnée après t-SNE 2D (temps: {duration:.2f}s)\", ax3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "- **133**: 1 ligne, 3 colonnes, 3ème position\n",
    "- Affiche la projection t-SNE avec:\n",
    "  - Couleurs par cluster original\n",
    "  - Temps d'exécution dans le titre\n",
    "- `tight_layout()` améliore l'espacement\n",
    "- `show()` affiche la figure\n",
    "\n",
    "## 6. Affichage des métriques\n",
    "```python\n",
    "print(\"\\nRésultats t-SNE:\")\n",
    "print(f\"Divergence KL finale: {tsne.kl_divergence_:.4f}\")\n",
    "print(f\"Itérations effectuées: {tsne.n_iter_}\")\n",
    "```\n",
    "- Affiche:\n",
    "  - La divergence KL (qualité de la projection)\n",
    "  - Le nombre réel d'itérations effectuées\n",
    "\n",
    "## 7. Exécution conditionnelle\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "- Garantit que le code ne s'exécute que si le script est lancé directement (pas en import)\n",
    "\n",
    "## Flux complet\n",
    "1. Génère des données 3D avec 4 clusters\n",
    "2. Montre:\n",
    "   - Une projection 2D naïve (perte d'information)\n",
    "   - La vue 3D originale (structure complète)\n",
    "   - La projection t-SNE 2D (préservation des clusters)\n",
    "3. Donne des métriques quantitatives\n",
    "\n",
    "## Résultat attendu\n",
    "- Visualisation comparative montrant comment t-SNE préserve mieux la structure des clusters qu'une simple projection 2D\n",
    "- Affichage des performances de l'algorithme\n",
    "- Démonstration complète du workflow t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebfc80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
